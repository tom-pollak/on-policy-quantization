# On-Policy Distillation for Model Quantization: A Null Result

Knowledge distillation (KD) is widely used for compressing large language models, training a smaller student to mimic a teacher's outputs. Standard KD methods train on fixed datasets, creating distribution mismatch—the student never learns from its own mistakes. On-policy distillation addresses this by having the student generate sequences and learning from teacher feedback on those generations, an approach shown effective for architectural distillation. We investigate whether on-policy distillation similarly benefits quantization, distilling from an FP16 teacher to an INT4 student.

Through extensive hyperparameter sweeps over learning rate, KL divergence direction (β), data source interpolation (λ), batch size, and rollout length, we find that on-policy distillation provides no significant benefit over standard off-policy training. Best-tuned on-policy (λ=1) and off-policy (λ=0) configurations differ by only 0.3% across five benchmarks. Extended training (20× compute) reduces loss by 42% but yields no eval improvement, suggesting we saturate INT4 precision limits. One notable finding: optimal β differs by regime—forward KL (β=0) for off-policy, reverse KL (β=1) for on-policy—aligning with theoretical predictions even when overall performance is equivalent. We hypothesize that distribution mismatch is minimal when student and teacher share weights at different precision, unlike architectural distillation where model capacity differs substantially. Given the compute overhead of on-policy rollouts, practitioners should prefer simpler off-policy approaches for quantization.
