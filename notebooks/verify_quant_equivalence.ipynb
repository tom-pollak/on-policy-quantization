{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Training vs Eval Quantization Equivalence\n",
    "\n",
    "This notebook checks that models are quantized identically during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:15:47.621298Z",
     "iopub.status.busy": "2026-01-12T10:15:47.621137Z",
     "iopub.status.idle": "2026-01-12T10:15:58.077617Z",
     "shell.execute_reply": "2026-01-12T10:15:58.076746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tomp/on-policy-distillation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TMA benchmarks will be running without grid constant TMA descriptor.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.qat import QATConfig\n",
    "from config import TrainConfig, EvalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:15:58.079433Z",
     "iopub.status.busy": "2026-01-12T10:15:58.079149Z",
     "iopub.status.idle": "2026-01-12T10:15:58.081884Z",
     "shell.execute_reply": "2026-01-12T10:15:58.081281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a small test config\n",
    "train_cfg = TrainConfig(quant_type=\"int4\")\n",
    "eval_cfg = EvalConfig(quant_type=\"int4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify QAT configs are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:15:58.083413Z",
     "iopub.status.busy": "2026-01-12T10:15:58.083265Z",
     "iopub.status.idle": "2026-01-12T10:15:58.086611Z",
     "shell.execute_reply": "2026-01-12T10:15:58.085954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training QAT config:\n",
      "  QATConfig(base_config=Int4WeightOnlyConfig(group_size=128, layout=TensorCoreTiledLayout(inner_k_tiles=8), use_hqq=False, zero_point_domain=<ZeroPointDomain.NONE: 3>, set_inductor_config=True, preserve_zero=None, int4_packing_format=<Int4PackingFormat.PLAIN: 'plain'>, int4_choose_qparams_algorithm=<Int4ChooseQParamsAlgorithm.TINYGEMM: 'tinygemm'>, version=2), activation_config=None, weight_config=None, step='prepare')\n",
      "\n",
      "Eval QAT config:\n",
      "  QATConfig(base_config=Int4WeightOnlyConfig(group_size=128, layout=TensorCoreTiledLayout(inner_k_tiles=8), use_hqq=False, zero_point_domain=<ZeroPointDomain.NONE: 3>, set_inductor_config=True, preserve_zero=None, int4_packing_format=<Int4PackingFormat.PLAIN: 'plain'>, int4_choose_qparams_algorithm=<Int4ChooseQParamsAlgorithm.TINYGEMM: 'tinygemm'>, version=2), activation_config=None, weight_config=None, step='prepare')\n",
      "\n",
      "✓ QAT configs match!\n"
     ]
    }
   ],
   "source": [
    "train_qat_config = train_cfg.get_qat_config()\n",
    "eval_qat_config = eval_cfg.get_qat_config()\n",
    "\n",
    "print(\"Training QAT config:\")\n",
    "print(f\"  {train_qat_config}\")\n",
    "\n",
    "print(\"\\nEval QAT config:\")\n",
    "print(f\"  {eval_qat_config}\")\n",
    "\n",
    "# Check they match\n",
    "assert str(train_qat_config) == str(eval_qat_config)\n",
    "print(\"\\n✓ QAT configs match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare model weights after QAT prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:15:58.087952Z",
     "iopub.status.busy": "2026-01-12T10:15:58.087811Z",
     "iopub.status.idle": "2026-01-12T10:16:03.085620Z",
     "shell.execute_reply": "2026-01-12T10:16:03.084735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|████████████████████████████████████████████████▎                                                                                                | 1/3 [00:01<00:02,  1.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 2/3 [00:02<00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|████████████████████████████████████████████████▎                                                                                                | 1/3 [00:01<00:02,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 2/3 [00:02<00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load two fresh models and apply QAT\n",
    "model_train = train_cfg.load_model()\n",
    "model_eval = eval_cfg.load_model()\n",
    "\n",
    "quantize_(model_train, train_cfg.get_qat_config())\n",
    "quantize_(model_eval, eval_cfg.get_qat_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:03.087564Z",
     "iopub.status.busy": "2026-01-12T10:16:03.087393Z",
     "iopub.status.idle": "2026-01-12T10:16:03.151539Z",
     "shell.execute_reply": "2026-01-12T10:16:03.150919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All parameters match between train_model and eval_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare all parameters\n",
    "def compare_models(m1, m2, name1=\"model1\", name2=\"model2\"):\n",
    "    \"\"\"Compare two models parameter by parameter.\"\"\"\n",
    "    mismatches = []\n",
    "    for (n1, p1), (n2, p2) in zip(m1.named_parameters(), m2.named_parameters()):\n",
    "        assert n1 == n2, f\"Parameter name mismatch: {n1} vs {n2}\"\n",
    "        if not torch.equal(p1, p2):\n",
    "            max_diff = (p1 - p2).abs().max().item()\n",
    "            mismatches.append((n1, max_diff))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"✗ {len(mismatches)} parameter mismatches between {name1} and {name2}:\")\n",
    "        for name, diff in mismatches[:10]:\n",
    "            print(f\"  {name}: max diff = {diff:.6e}\")\n",
    "    else:\n",
    "        print(f\"✓ All parameters match between {name1} and {name2}\")\n",
    "    return len(mismatches) == 0\n",
    "\n",
    "compare_models(model_train, model_eval, \"train_model\", \"eval_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare forward pass outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:03.152959Z",
     "iopub.status.busy": "2026-01-12T10:16:03.152815Z",
     "iopub.status.idle": "2026-01-12T10:16:04.290289Z",
     "shell.execute_reply": "2026-01-12T10:16:04.289347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit diff: max=0.000000e+00, mean=0.000000e+00\n",
      "✓ Forward pass outputs match!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(train_cfg.model_name)\n",
    "test_input = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(model_train.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_train = model_train(**test_input)\n",
    "    out_eval = model_eval(**test_input)\n",
    "\n",
    "logit_diff = (out_train.logits - out_eval.logits).abs()\n",
    "print(f\"Logit diff: max={logit_diff.max():.6e}, mean={logit_diff.mean():.6e}\")\n",
    "\n",
    "if logit_diff.max() < 1e-5:\n",
    "    print(\"✓ Forward pass outputs match!\")\n",
    "else:\n",
    "    print(\"✗ Forward pass outputs differ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:04.291851Z",
     "iopub.status.busy": "2026-01-12T10:16:04.291691Z",
     "iopub.status.idle": "2026-01-12T10:16:04.300095Z",
     "shell.execute_reply": "2026-01-12T10:16:04.299259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model_train, model_eval\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test full eval pipeline with a trained checkpoint\n",
    "\n",
    "Compare the training student model (before final save) with eval's reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:04.301558Z",
     "iopub.status.busy": "2026-01-12T10:16:04.301407Z",
     "iopub.status.idle": "2026-01-12T10:16:09.496187Z",
     "shell.execute_reply": "2026-01-12T10:16:09.495135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|████████████████████████████████████████████████▎                                                                                                | 1/3 [00:01<00:02,  1.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 2/3 [00:02<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in eval style (QAT prepare → LoRA → merge)\n",
      "First layer weight dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Point to a trained checkpoint\n",
    "CHECKPOINT_PATH = \"../dump/lmbda0_lr5e-6_beta0/checkpoint-1000\"  # adjust as needed\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}, skipping this test\")\n",
    "else:\n",
    "    # Simulate eval.py's loading (lines 260-269)\n",
    "    model_eval_style = eval_cfg.load_model()\n",
    "    quantize_(model_eval_style, eval_cfg.get_qat_config())\n",
    "    model_eval_style = PeftModel.from_pretrained(model_eval_style, CHECKPOINT_PATH)\n",
    "    model_eval_style = model_eval_style.merge_and_unload()\n",
    "    \n",
    "    print(\"Model loaded in eval style (QAT prepare → LoRA → merge)\")\n",
    "    print(f\"First layer weight dtype: {model_eval_style.model.layers[0].self_attn.q_proj.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check: Does convert step change outputs?\n",
    "\n",
    "Compare QAT fake-quant vs real INT4 after convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:09.498480Z",
     "iopub.status.busy": "2026-01-12T10:16:09.498028Z",
     "iopub.status.idle": "2026-01-12T10:16:12.995935Z",
     "shell.execute_reply": "2026-01-12T10:16:12.995107Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|████████████████████████████████████████████████▎                                                                                                | 1/3 [00:01<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 2/3 [00:02<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake quant model (training):\n",
      "  q_proj type: <class 'torchao.quantization.qat.linear.FakeQuantizedLinear'>\n",
      "\n",
      "Real int4 model (after convert):\n",
      "  q_proj type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Load fresh model with QAT prepare (fake quant)\n",
    "model_fake = train_cfg.load_model()\n",
    "quantize_(model_fake, train_cfg.get_qat_config())\n",
    "\n",
    "# Clone and convert to real int4\n",
    "model_real = copy.deepcopy(model_fake)\n",
    "quantize_(model_real, QATConfig(train_cfg._get_torchao_config(), step=\"convert\"))\n",
    "\n",
    "print(\"Fake quant model (training):\")\n",
    "print(f\"  q_proj type: {type(model_fake.model.layers[0].self_attn.q_proj)}\")\n",
    "\n",
    "print(\"\\nReal int4 model (after convert):\")\n",
    "print(f\"  q_proj type: {type(model_real.model.layers[0].self_attn.q_proj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:16:12.997592Z",
     "iopub.status.busy": "2026-01-12T10:16:12.997432Z",
     "iopub.status.idle": "2026-01-12T10:16:13.187001Z",
     "shell.execute_reply": "2026-01-12T10:16:13.186282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake vs Real logit diff: max=3.437500e-01, mean=5.834961e-02\n",
      "Predictions match: True\n"
     ]
    }
   ],
   "source": [
    "# Compare forward pass\n",
    "test_input = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(model_fake.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_fake = model_fake(**test_input)\n",
    "    out_real = model_real(**test_input)\n",
    "\n",
    "logit_diff = (out_fake.logits - out_real.logits).abs()\n",
    "print(f\"Fake vs Real logit diff: max={logit_diff.max():.6e}, mean={logit_diff.mean():.6e}\")\n",
    "\n",
    "# Check if predictions match\n",
    "pred_fake = out_fake.logits.argmax(-1)\n",
    "pred_real = out_real.logits.argmax(-1)\n",
    "print(f\"Predictions match: {(pred_fake == pred_real).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all checks pass:\n",
    "- Training and eval use identical quantization configs\n",
    "- Fresh models quantize identically\n",
    "- The `convert` step (fake → real int4) preserves model behavior\n",
    "\n",
    "If checks fail, investigate:\n",
    "1. Different random seeds affecting initialization\n",
    "2. QAT config differences between train/eval code paths\n",
    "3. LoRA merge order affecting quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
