{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Training vs Eval Quantization Equivalence\n",
    "\n",
    "This notebook checks that models are quantized identically during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.qat import QATConfig\n",
    "from config import TrainConfig, EvalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small test config\n",
    "train_cfg = TrainConfig(quant_type=\"int4\")\n",
    "eval_cfg = EvalConfig(quant_type=\"int4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify QAT configs are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_qat_config = train_cfg.get_qat_config()\neval_qat_config = eval_cfg.get_qat_config()\n\nprint(\"Training QAT config:\")\nprint(f\"  {train_qat_config}\")\n\nprint(\"\\nEval QAT config:\")\nprint(f\"  {eval_qat_config}\")\n\n# Check they match\nassert str(train_qat_config) == str(eval_qat_config)\nprint(\"\\n✓ QAT configs match!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare model weights after QAT prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two fresh models and apply QAT\n",
    "model_train = train_cfg.load_model()\n",
    "model_eval = eval_cfg.load_model()\n",
    "\n",
    "quantize_(model_train, train_cfg.get_qat_config())\n",
    "quantize_(model_eval, eval_cfg.get_qat_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all parameters\n",
    "def compare_models(m1, m2, name1=\"model1\", name2=\"model2\"):\n",
    "    \"\"\"Compare two models parameter by parameter.\"\"\"\n",
    "    mismatches = []\n",
    "    for (n1, p1), (n2, p2) in zip(m1.named_parameters(), m2.named_parameters()):\n",
    "        assert n1 == n2, f\"Parameter name mismatch: {n1} vs {n2}\"\n",
    "        if not torch.equal(p1, p2):\n",
    "            max_diff = (p1 - p2).abs().max().item()\n",
    "            mismatches.append((n1, max_diff))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"✗ {len(mismatches)} parameter mismatches between {name1} and {name2}:\")\n",
    "        for name, diff in mismatches[:10]:\n",
    "            print(f\"  {name}: max diff = {diff:.6e}\")\n",
    "    else:\n",
    "        print(f\"✓ All parameters match between {name1} and {name2}\")\n",
    "    return len(mismatches) == 0\n",
    "\n",
    "compare_models(model_train, model_eval, \"train_model\", \"eval_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare forward pass outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(train_cfg.model_name)\n",
    "test_input = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(model_train.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_train = model_train(**test_input)\n",
    "    out_eval = model_eval(**test_input)\n",
    "\n",
    "logit_diff = (out_train.logits - out_eval.logits).abs()\n",
    "print(f\"Logit diff: max={logit_diff.max():.6e}, mean={logit_diff.mean():.6e}\")\n",
    "\n",
    "if logit_diff.max() < 1e-5:\n",
    "    print(\"✓ Forward pass outputs match!\")\n",
    "else:\n",
    "    print(\"✗ Forward pass outputs differ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model_train, model_eval\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test full eval pipeline with a trained checkpoint\n",
    "\n",
    "Compare the training student model (before final save) with eval's reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to a trained checkpoint\n",
    "CHECKPOINT_PATH = \"../dump/lmbda0_lr5e-6_beta0/checkpoint-1000\"  # adjust as needed\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}, skipping this test\")\n",
    "else:\n",
    "    # Simulate eval.py's loading (lines 260-269)\n",
    "    model_eval_style = eval_cfg.load_model()\n",
    "    quantize_(model_eval_style, eval_cfg.get_qat_config())\n",
    "    model_eval_style = PeftModel.from_pretrained(model_eval_style, CHECKPOINT_PATH)\n",
    "    model_eval_style = model_eval_style.merge_and_unload()\n",
    "    \n",
    "    print(\"Model loaded in eval style (QAT prepare → LoRA → merge)\")\n",
    "    print(f\"First layer weight dtype: {model_eval_style.model.layers[0].self_attn.q_proj.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check: Does convert step change outputs?\n",
    "\n",
    "Compare QAT fake-quant vs real INT4 after convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Load fresh model with QAT prepare (fake quant)\n",
    "model_fake = train_cfg.load_model()\n",
    "quantize_(model_fake, train_cfg.get_qat_config())\n",
    "\n",
    "# Clone and convert to real int4\n",
    "model_real = copy.deepcopy(model_fake)\n",
    "quantize_(model_real, QATConfig(train_cfg._get_torchao_config(), step=\"convert\"))\n",
    "\n",
    "print(\"Fake quant model (training):\")\n",
    "print(f\"  q_proj type: {type(model_fake.model.layers[0].self_attn.q_proj)}\")\n",
    "\n",
    "print(\"\\nReal int4 model (after convert):\")\n",
    "print(f\"  q_proj type: {type(model_real.model.layers[0].self_attn.q_proj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare forward pass\n",
    "test_input = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(model_fake.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_fake = model_fake(**test_input)\n",
    "    out_real = model_real(**test_input)\n",
    "\n",
    "logit_diff = (out_fake.logits - out_real.logits).abs()\n",
    "print(f\"Fake vs Real logit diff: max={logit_diff.max():.6e}, mean={logit_diff.mean():.6e}\")\n",
    "\n",
    "# Check if predictions match\n",
    "pred_fake = out_fake.logits.argmax(-1)\n",
    "pred_real = out_real.logits.argmax(-1)\n",
    "print(f\"Predictions match: {(pred_fake == pred_real).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all checks pass:\n",
    "- Training and eval use identical quantization configs\n",
    "- Fresh models quantize identically\n",
    "- The `convert` step (fake → real int4) preserves model behavior\n",
    "\n",
    "If checks fail, investigate:\n",
    "1. Different random seeds affecting initialization\n",
    "2. QAT config differences between train/eval code paths\n",
    "3. LoRA merge order affecting quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}