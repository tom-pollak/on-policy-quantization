import logging
import os
from pathlib import Path

# Set cache directories before importing HF/lm_eval libraries
os.environ["HF_HOME"] = "./hf-cache"
os.environ["LM_HARNESS_CACHE_PATH"] = "./lm-eval-cache"
os.environ["HF_DATASETS_OFFLINE"] = "1"

import torch
import wandb
from accelerate import PartialState
from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM
from peft import PeftModel
from pydantic import validate_call
from pydantic_config import parse_argv
from torchao.quantization import quantize_
from transformers import AutoModelForCausalLM, AutoTokenizer

from config import EvalConfig, Tee


def get_latest_checkpoint(output_dir: Path) -> tuple[Path, int]:
    """Find the latest checkpoint in the given output directory."""
    checkpoint = max(
        output_dir.glob("checkpoint-*"), key=lambda p: int(p.name.split("-")[1])
    )
    step = int(checkpoint.name.split("-")[1])
    return checkpoint, step


def load_model(
    path: str,
    dtype: torch.dtype,
    quant_config=None,
    base_model: str | None = None,
):
    """Load a model, optionally with quantization via torchao.

    If base_model is provided, path is treated as a LoRA adapter directory.
    The adapter is merged before quantization.
    """
    if base_model is not None:
        # Load base model in full precision, merge LoRA, then quantize if needed
        model = AutoModelForCausalLM.from_pretrained(base_model, dtype=dtype)
        model = PeftModel.from_pretrained(model, path)
        model = model.merge_and_unload()
    else:
        model = AutoModelForCausalLM.from_pretrained(path, dtype=dtype)

    if quant_config is not None:
        quantize_(model, quant_config)
    return model


def run_lm_eval(
    model,
    tokenizer,
    task_list: list[str],
    num_fewshot: int | None = None,
) -> dict:
    """Run lm-evaluation-harness on specified tasks."""
    lm = HFLM(pretrained=model, tokenizer=tokenizer)
    results = evaluator.simple_evaluate(
        model=lm,
        tasks=task_list,
        num_fewshot=num_fewshot,
        batch_size="auto",
    )
    return {
        task: results["results"][task].get(
            "acc,none", results["results"][task].get("acc_norm,none")
        )
        for task in task_list
    }


@validate_call
def main(cfg: EvalConfig) -> None:
    state = PartialState()
    own_wandb_run = wandb.run is None

    if state.is_main_process:
        if own_wandb_run:
            wandb.init(project=cfg.wandb_project, name="eval", tags=cfg.tags)
        Tee.redirect_stdout_stderr("./eval.log")
    else:
        logging.disable(logging.WARNING)
        os.environ["TQDM_DISABLE"] = "1"

    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # header
    if state.is_main_process:
        print(f"{'model':25s} | " + " | ".join(f"{t[:8]:>8s}" for t in cfg.tasks))
        table = wandb.Table(columns=["model"] + cfg.tasks + ["avg"])
    else:
        table = None

    def eval_and_log(name: str, model):
        res = run_lm_eval(model, tokenizer, cfg.tasks)
        if state.is_main_process:
            assert table is not None
            avg = sum(res.values()) / len(res)
            print(
                f"{name:25s} | "
                + " | ".join(f"{res[t]:8.4f}" for t in cfg.tasks)
                + f" | {avg:8.4f}"
            )
            table.add_data(name, *[res[t] for t in cfg.tasks], avg)

            # Log individual metrics to summary for cross-run comparison
            for task, acc in res.items():
                wandb.summary[f"eval/{name}/{task}"] = acc
            wandb.summary[f"eval/{name}/avg"] = avg

        del model

    if cfg.eval_teacher:
        # Teacher model (unquantized)
        eval_and_log("teacher", cfg.load_model())
        torch.cuda.empty_cache()

        # Teacher model (quantized) - PTQ baseline
        eval_and_log(
            "teacher_ptq",
            cfg.load_quant_model("ptq"),
        )
        torch.cuda.empty_cache()

    # Evaluate each LoRA adapter
    for lora_path in cfg.lora_paths:
        checkpoint, step = get_latest_checkpoint(lora_path)
        model = cfg.load_model()
        model = PeftModel.from_pretrained(model, str(checkpoint))
        model = model.merge_and_unload()
        quantize_(model, cfg._get_torchao_config())
        eval_and_log(f"{lora_path.stem}/{step}", model)
        del model
        torch.cuda.empty_cache()

    if state.is_main_process:
        wandb.log({"eval_results": table})
        if own_wandb_run:
            wandb.finish()


if __name__ == "__main__":
    main(EvalConfig(**parse_argv()))
